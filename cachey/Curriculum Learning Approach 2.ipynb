{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/azibit/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2021-04-06 09:52:24,220\tWARNING deprecation.py:34 -- DeprecationWarning: `ray.rllib.env.atari_wrappers....` has been deprecated. Use `ray.rllib.env.wrappers.atari_wrappers....` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "## Import All Needed Libraries\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import ray\n",
    "from animalai.envs.arena_config import ArenaConfig\n",
    "from animalai.envs.gym.environment import AnimalAIGym\n",
    "from ray.rllib.agents import ppo\n",
    "\n",
    "from ray.tune import register_env, tune\n",
    "\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from cache_model import *\n",
    "from config import get_cfg\n",
    "from custom_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reuse Wrapper for AnimalAI Environment\n",
    "\n",
    "class UnityEnvWrapper(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.vector_index = env_config.vector_index\n",
    "        self.worker_index = env_config.worker_index\n",
    "        self.worker_id = env_config[\"unity_worker_id\"] + env_config.worker_index\n",
    "        self.env = AnimalAIGym(\n",
    "            environment_filename = \"../examples/env/AnimalAI\",\n",
    "            worker_id = self.worker_id,\n",
    "            flatten_branched = True,\n",
    "            uint8_visual = True,\n",
    "            arenas_configurations = ArenaConfig(env_config['arena_to_train'])\n",
    "        )\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "        \n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-06 09:58:01,088\tINFO services.py:1174 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "arena_configurations = ['0.yml', '1.yml', '2.yml', '3.yml', '4.yml']\n",
    "int phase = 0\n",
    "def on_train_result(info):\n",
    "    result = info[\"result\"]\n",
    "    if result[\"episode_reward_mean\"] > 0.9 and phase == 2:\n",
    "        print(\"<<<<<<<<<<<<<< PHASE 3 >>>>>>>>>>>>>>>>>>\")\n",
    "        phase = 3\n",
    "    elif result[\"episode_reward_mean\"] > 0.85 and phase == 1:\n",
    "        print(\"<<<<<<<<<<<<<< PHASE 2 >>>>>>>>>>>>>>>>>>\")\n",
    "        phase = 2\n",
    "    elif result[\"episode_reward_mean\"] > 0.82 and phase == 0:\n",
    "        print(\"<<<<<<<<<<<<<< PHASE 1 >>>>>>>>>>>>>>>>>>\")\n",
    "        phase = 1\n",
    "    elif result[\"episode_reward_mean\"] > 0.80 and phase == 0:\n",
    "        print(\"<<<<<<<<<<<<<< PHASE 0 WITH PROGRESS >>>>>>>>>>>>>>>>>>\")\n",
    "        phase = 0\n",
    "    else:\n",
    "        print(\"<<<<<<<<<<<<<< PHASE 0 >>>>>>>>>>>>>>>>>>\")\n",
    "#         phase = 0\n",
    "    trainer.workers.foreach_worker(lambda ev: ev.foreach_env(\n",
    "                lambda env: env.env._env.reset(arenas_configurations=\n",
    "                                              ArenaConfig('../examples/configurations/curriculum/' + arena_configurations[phase]))))\n",
    "    trainer.save()\n",
    "\n",
    "## Setup configuration to use\n",
    "conf = {\n",
    "        \"num_workers\": 0,\n",
    "        \"env\": \"unity_env\",\n",
    "        \"callbacks\": {\n",
    "            \"on_train_result\": on_train_result,\n",
    "        },\n",
    "        \"env_config\": {\n",
    "            \"unity_worker_id\": 100,\n",
    "            \"arena_to_train\": '../examples/configurations/curriculum/0.yml',\n",
    "            },\n",
    "            \"model\": {\n",
    "                \"custom_model\": 'my_cnn_rnn_model',\n",
    "                \"custom_model_config\": {},\n",
    "            },\n",
    "            \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"2\")),\n",
    "            \"num_workers\": 2,  # parallelism\n",
    "            \"framework\": \"torch\",\n",
    "            \"train_batch_size\": 500\n",
    "        }\n",
    "    \n",
    "    \n",
    "## Setup and register environment\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "# Register custom models so that we can give the ID to the policy trainer\n",
    "# ModelCatalog.register_custom_model(\"my_fc_model\", MyFCForwardModel)\n",
    "ModelCatalog.register_custom_model(\"my_rnn_model\", MyRNNModel)\n",
    "ModelCatalog.register_custom_model(\"my_convgru_model\", MyConvGRUModel)  # NOTE: Only works with image observations.\n",
    "ModelCatalog.register_custom_model(\"my_cnn_rnn_model\", MyCNNRNNModel)\n",
    "\n",
    "register_env(\"unity_env\", lambda config: UnityEnvWrapper(config))\n",
    "    \n",
    "# ## Setup trainer\n",
    "# trainer = PPOTrainer(config=conf, env= \"unity_env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TuneError",
     "evalue": "Insufficient cluster resources to launch trial: trial requested 2 CPUs, 2 GPUs, but the cluster has only 8 CPUs, 0 GPUs, 6.93 GiB heap, 2.39 GiB objects (1.0 node:192.168.1.4). \n\nYou can adjust the resource requests of RLlib agents by setting `num_workers`, `num_gpus`, and other configs. See the DEFAULT_CONFIG defined by each agent for more info.\n\nThe config of this agent is: {'num_workers': 1, 'env': 'unity_env', 'callbacks': {'on_train_result': <function on_train_result at 0x7f080fb23f80>}, 'env_config': {'unity_worker_id': 100, 'arena_to_train': '../examples/configurations/curriculum/0.yml'}, 'model': {'custom_model': 'my_cnn_rnn_model', 'custom_model_config': {}}, 'num_gpus': 2, 'framework': 'torch', 'train_batch_size': 500} ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fd234b48f974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m result = tune.run(\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"PPO\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0mtune_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVerbosity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1_EXPERIMENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_experiment_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/tune/trial_executor.py\u001b[0m in \u001b[0;36mon_no_available_trials\u001b[0;34m(self, trial_runner)\u001b[0m\n\u001b[1;32m    184\u001b[0m                         )\n\u001b[1;32m    185\u001b[0m                     raise TuneError(\n\u001b[0;32m--> 186\u001b[0;31m                         \u001b[0;34m\"Insufficient cluster resources to launch trial: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                         \u001b[0;34mf\"trial requested {resource_string}, but the cluster \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                         \u001b[0;34mf\"has only {self.resource_string()}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: Insufficient cluster resources to launch trial: trial requested 2 CPUs, 2 GPUs, but the cluster has only 8 CPUs, 0 GPUs, 6.93 GiB heap, 2.39 GiB objects (1.0 node:192.168.1.4). \n\nYou can adjust the resource requests of RLlib agents by setting `num_workers`, `num_gpus`, and other configs. See the DEFAULT_CONFIG defined by each agent for more info.\n\nThe config of this agent is: {'num_workers': 1, 'env': 'unity_env', 'callbacks': {'on_train_result': <function on_train_result at 0x7f080fb23f80>}, 'env_config': {'unity_worker_id': 100, 'arena_to_train': '../examples/configurations/curriculum/0.yml'}, 'model': {'custom_model': 'my_cnn_rnn_model', 'custom_model_config': {}}, 'num_gpus': 2, 'framework': 'torch', 'train_batch_size': 500} "
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "\n",
    "# ray.init()\n",
    "result = tune.run(\n",
    "    \"PPO\",\n",
    "    config=conf,\n",
    "    resources_per_trial={\n",
    "            \"cpu\": 5,\n",
    "            \"gpu\": 2,\n",
    "            \"extra_cpu\": 2,\n",
    "        },\n",
    "    stop={\"training_iteration\": 1000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
