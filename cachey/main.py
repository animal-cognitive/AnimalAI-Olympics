import ray
from ray.rllib.agents.ppo import PPOTrainer
from ray.rllib.models import ModelCatalog
from ray.tune.logger import pretty_print

from cache_model import *
from config import get_cfg
from custom_model import *

from animalai.envs.gym.environment import AnimalAIGym
import os
from ray.tune import register_env, tune

class UnityEnvWrapper(gym.Env):
    def __init__(self, env_config):
        self.vector_index = env_config.vector_index
        self.worker_index = env_config.worker_index
        self.worker_id = env_config["unity_worker_id"] + env_config.worker_index
        self.env = AnimalAIGym(
            environment_filename = "../examples/env/AnimalAI",
            worker_id = self.worker_id,
            flatten_branched = True,
            uint8_visual = True,
            arenas_configurations = ArenaConfig('../examples/configurations/curriculum/0.yml')
        )
        self.action_space = self.env.action_space
        self.observation_space = self.env.observation_space

    def reset(self):
        return self.env.reset()

    def step(self, action):
        return self.env.step(action)

# Register custom models so that we can give the ID to the policy trainer
# ModelCatalog.register_custom_model("my_fc_model", MyFCForwardModel)
ModelCatalog.register_custom_model("my_rnn_model", MyRNNModel)
ModelCatalog.register_custom_model("my_convgru_model", MyConvGRUModel)  # NOTE: Only works with image observations.
ModelCatalog.register_custom_model("my_cnn_rnn_model", MyCNNRNNModel)


def train(cfg):
    """
    Train a model with the given config
    """

    import torch
    print('CUDA available:', torch.cuda.is_available())
    ray.init(include_dashboard=False)

    # Transform our config to a Ray config
    ray_cfg = {
        "env": cfg["env_id"],
        "num_gpus": 0,
        "num_workers": 0,
        "framework": 'torch',
        "model": {
            "custom_model": 'my_cnn_rnn_model',
        },
        "log_level": 'WARN',
    }

    """Use Ray Tune to train an agent.
    Configure cfg["stop"] for what criterion to use to stop training (e.g. timesteps_total, episode_reward_mean).
    https://docs.ray.io/en/master/tune/index.html
    The results will be saved in ./ray_results and can be loaded by instantiating an Analysis.
    https://docs.ray.io/en/master/tune/api_docs/analysis.html#analysis-tune-analysis
    """
    analysis = ray.tune.run(
        PPOTrainer,
        config=ray_cfg,
        local_dir=cfg["local_dir"],
        stop=cfg["stop"],
        checkpoint_freq=10,
        keep_checkpoints_num=6,
        checkpoint_at_end=True)
    print(analysis)


def example_instrument(cfg):
    """Added a flag so I can debug easier (without having to reason about Tune)"""
    trainer = PPOTrainer(env=cfg["env_id"], config={"model": {
        "custom_model": 'my_rnn_model',
    },
        "framework": 'torch',
        "num_workers": 0,
    })
    model = trainer.get_policy().model

    """
    Prepare a dummy batch of 32 images (B x W x H), assuming one sequence.
    The dimensions of LSTM input and output are (B x T x F), for B=Batch, T=Time, F=Features.
    A batch is a set of sequences, where a sequence is a list of observations.
    Each sequence must be the same length, so Ray pads 0's to fill out shorter sequences to the same length.
    `seq_lens` is a list of the length (in steps) of each sequence.
    Example:
        obs: torch.tensor((32, 84, 84))
        seq_lens: [8, 6, 4, 8]
        input to model: torch.tensor((8, 4, 84, 84))
    All sequences must be length 8, so the second and third sequences are padded with 0's for the missing observations.
    """
    # This is how a batch of observations looks coming out of the environment.
    # 32 images, each 84 x 84 pixels
    # obs = torch.zeros((32, 84, 84)).cuda()
    obs = torch.zeros((32, 84, 84))
    # This input dict would be generated by rllib in ModelV2.__call__
    # (which wraps our custom model's forward() function)
    input_dict = {
        "obs_flat": torch.flatten(obs, start_dim=1),
    }

    # Get a dummy initial LSTM state using the utility method built into our custom model.
    state = model.get_initial_state()
    # Unsqueeze to encode 1 sequence. This would be handled by rllib.
    # state = [s.cuda().unsqueeze(dim=0) for s in state]
    state = [s.unsqueeze(dim=0) for s in state]
    # 1 sequence of length 32.
    seq_lens = torch.tensor([32])

    # Register hook to be called after forward()
    def print_dir_shape(self, input, output):
        """Example hook that just prints the input/output shapes."""
        in_features, in_state = input
        out_features, out_state = output
        # Expect (1 x 1 x 64) coming from FC layer into LSTM
        print('input features:', in_features.shape, 'previous state:', [s.shape for s in in_state])
        # Expect (1 x 1 x 256) coming out of LSTM. This is the DIR.
        print('DIR output:', out_features.shape, 'current state:', [s.shape for s in out_state])

    model.lstm.register_forward_hook(print_dir_shape)

    # Pass observations to our model
    model.forward(input_dict, state, seq_lens)

def test_trainonebatch():
    """Test hooking into an RLLib trainer."""
    # ray.shutdown()
    # ray.init(include_dashboard=False)

    conf = {"env": "unity_env",
            "num_workers": 0,
            "env_config": {
                "unity_worker_id": 52
            },
            "model": {
                    "custom_model": 'my_cnn_rnn_model',
                    "custom_model_config": {},
                },
        "num_gpus": int(os.environ.get("RLLIB_NUM_GPUS", "0")),
        "num_workers": 1,  # parallelism
        "framework": "torch",
        "train_batch_size": 500
       }

    ray.shutdown()
    ray.init()

    register_env("unity_env", lambda config: UnityEnvWrapper(config))
    result = tune.run("PPO", stop = {"timesteps_total": 1000, "training_iteration": 3}, config = conf, checkpoint_at_end = True)
    # register_env("my_env", lambda c: RayAIIGym(c, ArenaConfig('../examples/configurations/curriculum/0.yml')))
    # ModelCatalog.register_custom_model("my_cnn_rnn_model", MyCNNRNNModel)
    # trainer = PPOTrainer(env="my_env", config={
    #     "model": {
    #         "custom_model": 'my_cnn_rnn_model',
    #         "custom_model_config": {},
    #     },
    #     "framework": 'torch',
    # })
    # trainer.train()


def main():
    cfg = get_cfg()
    # example_instrument(cfg)
    test_trainonebatch()
    if cfg["train"]:
        train(cfg)


if __name__ == '__main__':
    main()
