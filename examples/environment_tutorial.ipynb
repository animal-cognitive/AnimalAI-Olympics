{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animal-AI Environment tutorial\n",
    "\n",
    "This tutorial is a step-by-step presentation of the new version of the Animal-AI library. The new Animal-AI environment is quite similar to the version used for the competition, however the `animalai` and `animalai_train` APIs have been dramatically improved, reflecting the great improvements made by [Unity ml-agents](https://github.com/Unity-Technologies/ml-agents).\n",
    "\n",
    "In this notebook, **we present the environment and discuss how to design both your training and testing setups**. In the second notebook (training) we'll show you how to train an agent to solve a task it has never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing animal cognition to the AI world\n",
    "\n",
    "Our goal is to provide a tool for researchers to go beyond classical RL environments, allowing you to develop agents that possess cogintive skills similar to animal's. The main idea is to be able to test and/or train your agents on **experiments taken or inspired from real life animal experiments**. This repository holds 900 such experiments which cover a dozen cognitive skills. You can find more details on the test-bed on [our website](http://animalaiolympics.com/AAI/testbed)\n",
    "\n",
    "The environment is a simple arena with an agent that can only move left, right, forward and backward, aiming to collect positive reward and avoid negative ones. It can also hold [several objects](https://github.com/beyretb/AnimalAI-Olympics/blob/master/documentation/definitionsOfObjects.md) which can be used to set up experiments. You can really put yourself in the shoes of an animal cognition scientist building experiments with whatever you can find in a lab.\n",
    "\n",
    "From the agent's perspective, a classical experiment called a Y-maze looks like this(the agent must explore a simple Y-shaped maze to find a reward, often food):\n",
    "\n",
    "<img src=\"notebook_data/y_maze.png\" width=\"40%\">\n",
    "\n",
    "The agent is on an elevated platform (blue), needs to move towards the reward (green ball) and avoid going to the right in which case the agent would be stuck (the platform is too high for the agent to climb back on).\n",
    "\n",
    "From an RL perspective this might seem like a trivial problem to solve! In a classical RL setup where you train and test on the same problem it is indeed simple. However, when tested on a similar task, an animal would encounter this problem for the first time. And this is what we encourage you to do as well: **create your own training curriculum, and use our experiments as a test set your agent has never seen before**. We believe this is needed to truly test an agent's capacity to acquire cognitive skills.\n",
    "\n",
    "\n",
    "But enough chit-chat, let's dive right in with an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Can your agent self control? - Part I\n",
    "\n",
    "Self control is hard, we've all been there (looking at you chocolate bar). But don't worry, this is something a lot of species struggle with. In [The evolution of self-control](https://www.pnas.org/content/111/20/E2140) MacLean et al. tested this ability in **36 different species**! In a very simple experiment, animals are offered food they can reach easily by reaching out to it. But then, they're shown the same food behind a transparent wall, they need to go around the wall to grab the food. They can see the food just as well, but they need to refrain from reaching out like before.\n",
    "\n",
    "Below are videos of such animals, as well as two participants' submissions to our compeition, exhibiting similar behaviors (remember, these agents never encoutered this task during training):                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div><video width=\"24%\" playsinline=\"\" autoplay=\"\" muted=\"\" loop=\"\"><source src=\"notebook_data/animal-cyl-pass.mp4\" type=\"video/mp4\"></video><video width=\"24%\" \" playsinline=\"\" autoplay=\"\" muted=\"\" loop=\"\"><source src=\"notebook_data/agent-cyl-pass.mp4\" type=\"video/mp4\"></video><video width=\"24%\" playsinline=\"\" autoplay=\"\" muted=\"\" loop=\"\"><source src=\"notebook_data/animal-cyl-fail.mp4\" type=\"video/mp4\"></video><video width=\"24%\" playsinline=\"\" autoplay=\"\" muted=\"\" loop=\"\"><source src=\"notebook_data/agent-cyl-fail.mp4\" type=\"video/mp4\"></video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<div><div><video width=\"24%\" playsinline=\"\" autoplay=\"\" muted=\"\" loop=\"\"><source src=\"notebook_data/animal-cyl-pass.mp4\" type=\"video/mp4\"></video><video width=\"24%\" \" playsinline=\"\" autoplay=\"\" muted=\"\" loop=\"\"><source src=\"notebook_data/agent-cyl-pass.mp4\" type=\"video/mp4\"></video><video width=\"24%\" playsinline=\"\" autoplay=\"\" muted=\"\" loop=\"\"><source src=\"notebook_data/animal-cyl-fail.mp4\" type=\"video/mp4\"></video><video width=\"24%\" playsinline=\"\" autoplay=\"\" muted=\"\" loop=\"\"><source src=\"notebook_data/agent-cyl-fail.mp4\" type=\"video/mp4\"></video></div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections we'll design a training curriculum which does not include the exact \"reward in a transparent cylinder\" task, but which we can use to train an agent that can solve this same task. In the training tutorial, we'll train such an agent using this curriculum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get started: experiments design\n",
    "\n",
    "First things first, as rigorous researchers, we want to design a good training environment. To do so, we provide a [list of items](https://github.com/beyretb/AnimalAI-Olympics/blob/master/examples/environment_tutorial.ipynb) you can include in your arena, you can have a look at the details later, this section highlights the basics.\n",
    "\n",
    "To begin with let's train an agent to collect food right in front of it, as simple as that! To do so, you'll need to design a `yaml` file which describes the experiment setup. It contains:\n",
    "\n",
    "- experiment parameters (maximum steps, steps at which the light is turned on/off)\n",
    "- a list of objects\n",
    "- their specifications (positions, rotations, sizes, colors) which are randomized if not provided\n",
    "\n",
    "Below is the simplest example possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!ArenaConfig\n",
      "arenas:\n",
      "  -1: !Arena\n",
      "    pass_mark: 0\n",
      "    t: 250\n",
      "    items:\n",
      "    - !Item\n",
      "      name: Agent\n",
      "      positions:\n",
      "      - !Vector3 {x: 20, y: 0, z: 20}\n",
      "      rotations: [0]\n",
      "    - !Item\n",
      "      name: GoodGoal\n",
      "      positions:\n",
      "      - !Vector3 {x: 20, y: 0, z: 22}\n",
      "      sizes:\n",
      "      - !Vector3 {x: 1, y: 1, z: 1}\n"
     ]
    }
   ],
   "source": [
    "with open('configurations/curriculum/0.yml') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the configuration of one arena (`!Arena`), with only the agent on the ground (`y=0`) in the center (`x=20`,`z=20`) and a `GoodGoal` (green sphere) of size 1 in front of it (`x=20`,`z=22`). Pretty simple right!\n",
    "\n",
    "One _little trick_ we used here: one environment can contain several arenas during training, each with its own configuration. This allows your training algorithm to collect more observations at once. You can just place the configurations one after the others like this:\n",
    "```\n",
    "!ArenaConfig\n",
    "arenas:\n",
    "  0: !Arena\n",
    "    ......\n",
    "  1: !Arena\n",
    "    ......\n",
    "  2: !Arena\n",
    "    ......\n",
    "```\n",
    "But if you want all the arenas in the environment to have the same configuration then do as we did above: define one configuration only with key `-1`\n",
    "\n",
    "You can now use this to load an environment and play yourself (`load_config_and_play.py` does that for you). Make sure you have followed the [installation guide](https://github.com/beyretb/AnimalAI-Olympics#requirements) and then create an `AnimalAIEnvironment` in play mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from animalai.envs.arena_config import ArenaConfig\n",
    "from animalai.envs.environment import AnimalAIEnvironment\n",
    "from mlagents_envs.exception import UnityCommunicationException\n",
    "\n",
    "try:\n",
    "    environment = AnimalAIEnvironment(\n",
    "            file_name='env/AnimalAI',\n",
    "            base_port=5005,\n",
    "            arenas_configurations=ArenaConfig('configurations/curriculum/0.yml'),\n",
    "            play=True,\n",
    "        )\n",
    "except UnityCommunicationException:\n",
    "    # you'll end up here if you close the environment window directly\n",
    "    # always try to close it from script\n",
    "    environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press **C** to change the viewpoint (bird's eye, first person, third person), and move with **W,A,S,D** or the **arrows** on your keyboard. Once you're done, let's close this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityEnvironmentException",
     "evalue": "No Unity environment is loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnityEnvironmentException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-55fdb398e359>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# takes a few seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/animal/lib/python3.7/site-packages/animalai/envs/environment.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animal/lib/python3.7/site-packages/mlagents_envs/environment.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnityEnvironmentException\u001b[0m: No Unity environment is loaded."
     ]
    }
   ],
   "source": [
    "if environment:\n",
    "    environment.close() # takes a few seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a curriculum\n",
    "\n",
    "Such a training set is not going to get us very far... The food is right before the agent, it won't even learn any sort of exploration - not even turning around to see if the food is behing it. [Curriculum learning](https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html) uses a set of training configurations of increasing difficulty in order to learn a complex task. Think \"stand up before you walk, walk before you run\" type of learning.\n",
    "\n",
    "To solve our problem, we might want to have the following consecutive learning steps:\n",
    "\n",
    "1. food right in front of the agent (example above)\n",
    "2. food in front of the agent, but further away\n",
    "3. food at the same distance as 2, but randomize the agent's rotation (might be behind the agent)\n",
    "4. agent and food randomly placed on a fixed z-axis, and a small transparent wall in between the two\n",
    "5. same as 4 with increasingly bigger walls\n",
    "\n",
    "To design a curriculum, we need to place all the yaml files in a folder along with a json configuration file which contains the details of when to switch from one level to the next. The above curriculum can be found in `configurations/curriculum`.\n",
    "\n",
    "The second configuration is just like the first but with `z: 35` for `GoodGoal`. The third one only requires randomizing the rotation; achieved by replacing `rotations: [0]` with `rotations: [-1]`, as any parameter with a value of `-1` is randomized. Otherwise you can just remove the `rotations` line altogether, and the rotation will be randomized automatically (also works with positions, sizes and colors).\n",
    "\n",
    "Putting all of the above together, we can have a look at step 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!ArenaConfig\n",
      "arenas:\n",
      "  -1: !Arena\n",
      "    pass_mark: 0\n",
      "    t: 250\n",
      "    items:\n",
      "    - !Item\n",
      "      name: Agent\n",
      "      positions:\n",
      "      - !Vector3 {x: -1, y: 0, z: 5}\n",
      "    - !Item\n",
      "      name: GoodGoal\n",
      "      positions:\n",
      "      - !Vector3 {x: -1, y: 0, z: 35}\n",
      "      sizes:\n",
      "      - !Vector3 {x: 1, y: 1, z: 1}\n",
      "    - !Item\n",
      "      name: WallTransparent\n",
      "      positions:\n",
      "      - !Vector3 {x: -1, y: 0, z: 20}\n",
      "      sizes:\n",
      "      - !Vector3 {x: 10, y: 5, z: 1}\n",
      "      rotations: [0]\n"
     ]
    }
   ],
   "source": [
    "configuration = 'configurations/curriculum/3.yml'\n",
    "with open(configuration) as f:\n",
    "    print(f.read())\n",
    "    \n",
    "try:\n",
    "    environment = AnimalAIEnvironment(\n",
    "            file_name='env/AnimalAI',\n",
    "            base_port=5005,\n",
    "            arenas_configurations=ArenaConfig(configuration),\n",
    "            play=True,\n",
    "        )\n",
    "except UnityCommunicationException:\n",
    "    # you'll end up here if you close the environment window directly\n",
    "    # always try to close it from script\n",
    "    environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play a few runs with this configuration, you'll see the various items randomly appearing along a given axis. You can **press R** to reset the environment. Once you're done, close the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if environment:\n",
    "    environment.close() # takes a few seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the file `AnimalAI.json` holds the parameters for the process (the name of this file **must remain AnimalAI.json**), and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"measure\": \"reward\",\n",
      "  \"thresholds\": [\n",
      "    0.8,\n",
      "    0.8,\n",
      "    0.8,\n",
      "    0.6,\n",
      "    0.2\n",
      "  ],\n",
      "  \"min_lesson_length\": 100,\n",
      "  \"signal_smoothing\": true,\n",
      "  \"configuration_files\": [\n",
      "    \"0.yml\",\n",
      "    \"1.yml\",\n",
      "    \"2.yml\",\n",
      "    \"3.yml\",\n",
      "    \"4.yml\",\n",
      "    \"5.yml\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('configurations/curriculum/AnimalAI.json') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells you that we'll switch from one level to the next once the reward per episod is above 0.8. Easy right?\n",
    "\n",
    "In the next notebook we'll use the above curriculum example to train an agent that can solve the tube task we saw in the videos earlier. Before that, it is worth looking at an extra feature of the environment (blackouts) as well as interacting with the environment from python rather than playing manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the environment + bonus light switch!\n",
    "\n",
    "In this final part, we look at the API for interacting with the environment. Namely, we want to take steps, collect observations and rewards. For this part we'll load an environment which tests for a cognitive skill called **object permanence**. It tests the capacity of an agent to understand that an object still exists even if it is moved out of sight; think of a car turning a corner - we all know the car hasn't vanished from existence. This test introduces another feature of the environment: **the light switch** which allows us to switch the light in the environment on and off. Let's have a look at the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_switch_conf = 'configurations/arena_configurations/light_switch.yml'\n",
    "\n",
    "try:\n",
    "    environment = AnimalAIEnvironment(\n",
    "            file_name='env/AnimalAI',\n",
    "            base_port=5005,\n",
    "            arenas_configurations=ArenaConfig(light_switch_conf),\n",
    "            play=True,\n",
    "        )\n",
    "except UnityCommunicationException:\n",
    "    # you'll end up here if you close the environment window directly\n",
    "    # always try to close it from script\n",
    "    environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the point of view by pressing **C** and then reset by pressing **R**. As you can see, the light switches off just before you can see the reward disappear, but you probably figured out where it had gone, right?\n",
    "\n",
    "This is achieved by adding the `blackouts` parameter to your configuration file. This consists of a list of frames at which the light should switch on, off, on, off... Below we switch the light off at frame 20 and then back on at 50. You can also **provide a negative number**, e.g. `blackouts: [-20]`, to switch on/off every 20 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!ArenaConfig\n",
      "arenas:\n",
      "  0: !Arena\n",
      "    pass_mark: 0\n",
      "    t: 500\n",
      "    blackouts: [20,50]\n",
      "    items:\n",
      "    - !Item\n",
      "      name: GoodGoalBounce\n",
      "      positions:\n",
      "      - !Vector3 {x: 30, y: 0, z: 30}\n",
      "      rotations: [270]\n",
      "      sizes:\n",
      "      - !Vector3 {x: 1, y: 1, z: 1}\n",
      "    - !Item\n",
      "      name: Wall\n",
      "      positions:\n",
      "      - !Vector3 {x: 7.5, y: 0, z: 25}\n",
      "      rotations: [90]\n",
      "      sizes:\n",
      "      - !Vector3 {x: 1, y: 3, z: 15}\n",
      "      colors:\n",
      "      - !RGB {r: 153, g: 153, b: 153}\n",
      "    - !Item\n",
      "      name: Ramp\n",
      "      positions:\n",
      "        - !Vector3 {x: 10, y: 0, z: 30}\n",
      "      rotations: [90]\n",
      "      sizes:\n",
      "        - !Vector3 {x: 1, y: 0.2, z: 1}\n",
      "      colors:\n",
      "        - !RGB {r: 153, g: 153, b: 153}\n",
      "    - !Item\n",
      "      name: Agent\n",
      "      positions:\n",
      "      - !Vector3 {x: 20, y: 0, z: 5}\n",
      "      rotations: [0]\n"
     ]
    }
   ],
   "source": [
    "if environment:\n",
    "    environment.close() # takes a few seconds\n",
    "    \n",
    "with open(light_switch_conf) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish this tutorial on the environment and associated API, we look at how we can interact with the environment from Python. To do so we'll launch the environment without play mode enabled, allowing a communicator between Python and Unity to exchange actions and observations. We will also set the camera resolution for our agent's observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityWorkerInUseException",
     "evalue": "Couldn't start socket communication because worker number 0 is still in use. You may need to manually close a previously opened environment or use a different worker number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/animal/lib/python3.7/site-packages/mlagents_envs/rpc_communicator.py\u001b[0m in \u001b[0;36mcheck_port\u001b[0;34m(self, port)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"localhost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 98] Address already in use",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnityWorkerInUseException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8075be199850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'env/AnimalAI'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mbase_port\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5007\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mresolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         )\n\u001b[1;32m      9\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mUnityCommunicationException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animal/lib/python3.7/site-packages/animalai/envs/environment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_name, worker_id, base_port, seed, n_arenas, play, arenas_configurations, inference, resolution, grayscale, side_channels)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mtimeout_wait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mside_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mside_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marenas_configurations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animal/lib/python3.7/site-packages/mlagents_envs/environment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_name, worker_id, base_port, seed, docker_training, no_graphics, timeout_wait, args, side_channels)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout_wait\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout_wait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_communicator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_port\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mside_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUUID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSideChannel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animal/lib/python3.7/site-packages/mlagents_envs/environment.py\u001b[0m in \u001b[0;36mget_communicator\u001b[0;34m(worker_id, base_port, timeout_wait)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_communicator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_port\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mRpcCommunicator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_port\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animal/lib/python3.7/site-packages/mlagents_envs/rpc_communicator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, worker_id, base_port, timeout_wait)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_open\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animal/lib/python3.7/site-packages/mlagents_envs/rpc_communicator.py\u001b[0m in \u001b[0;36mcreate_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mCreates\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mGRPC\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_port\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/animal/lib/python3.7/site-packages/mlagents_envs/rpc_communicator.py\u001b[0m in \u001b[0;36mcheck_port\u001b[0;34m(self, port)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"localhost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityWorkerInUseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnityWorkerInUseException\u001b[0m: Couldn't start socket communication because worker number 0 is still in use. You may need to manually close a previously opened environment or use a different worker number."
     ]
    }
   ],
   "source": [
    "resolution=256 # Resolution of agent's observations\n",
    "\n",
    "try:\n",
    "    environment = AnimalAIEnvironment(\n",
    "            file_name='env/AnimalAI',\n",
    "            base_port=5007,\n",
    "            resolution=resolution,\n",
    "        )\n",
    "except UnityCommunicationException:\n",
    "    # you'll end up here if you close the environment window directly\n",
    "    # always try to close it from script\n",
    "    environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first retrieve the various caracteristics of the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_groups = environment.get_agent_groups()\n",
    "agent_group_spec = environment.get_agent_group_spec(agent_groups[0])\n",
    "\n",
    "print(f'Here we only have {len(agent_groups)} agent group: {agent_groups[0]}')\n",
    "print(f'''\\nAnd you can get their caracteristics: \\n\n",
    "visual observations shape: {agent_group_spec.observation_shapes[0]}\n",
    "vector observations shape (velocity): {agent_group_spec.observation_shapes[1]}\n",
    "actions are discrete: {agent_group_spec.action_type}\n",
    "actions have shape: {agent_group_spec.action_shape}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we did not pass an arena configuration file to the environment  - you can actually pass one to the environment at any point during training when you reset it. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.reset(arenas_configurations=ArenaConfig(light_switch_conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can now take actions and collect observations and rewards! Here, the rollout and analysis are performed in two separate steps. Note that Unity allows agents to request actions only when they choose (unlike in Gym), although that's not relevent for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "actions = [[0,0]]*50 # Do nothing until the lights come back on\n",
    "actions += [[1,0]]*40 # Go forward\n",
    "actions += [[0,2]]*15 # turn left\n",
    "actions += [[1,0]]*50 # go forward again\n",
    "\n",
    "agent_group = agent_groups[0] \n",
    "visual_observations = []\n",
    "velocity_observations = []\n",
    "rewards = []\n",
    "done = False\n",
    "step = 0\n",
    "\n",
    "while not done and step<len(actions):\n",
    "    action = np.array(actions[step]).reshape(1,2)\n",
    "    \n",
    "    environment.set_actions(agent_group=agent_group, action=action)\n",
    "    environment.step()\n",
    "    step_result = environment.get_step_result(agent_group)\n",
    "    \n",
    "    visual_observations.append(step_result.obs[0])\n",
    "    velocity_observations.append(step_result.obs[1])\n",
    "    done = step_result.done[0]\n",
    "    rewards.append(step_result.reward[0])\n",
    "    max_step_reached = step_result.max_step[0]\n",
    "    step+=1\n",
    "# environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry if you didn't see anything happening in the Unity window. To conclude let's look at what we retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "vals  = ax.imshow(np.zeros((resolution, resolution,3)))\n",
    "cumulative_reward = 0\n",
    "# visual_observations[0].shape\n",
    "for i in range(len(visual_observations)):\n",
    "    \n",
    "    vals.set_data(visual_observations[i].reshape((resolution, resolution,3)))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "    \n",
    "    cumulative_reward+=rewards[i]\n",
    "    print(f'Step {i}')\n",
    "    print(f'Forward velocity \\t {velocity_observations[i][0][2]:0.2f}')\n",
    "    print(f'Right velocity \\t\\t {velocity_observations[i][0][0]:0.2f}')\n",
    "    print(f'Up velocity \\t\\t {velocity_observations[i][0][1]:0.2f}')\n",
    "    print(f'Cumulative reward {cumulative_reward:0.2f}')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if environment:\n",
    "    environment.close() # takes a few seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty much it! In summary, we provide a very efficient environments manager (derived from ml-agents) which allows the training loop to be run over several instances of the environment, yielding improved performance. You can also use the Gym implementation of the environment - see the examples on training with OpenAI baselines for more information.\n",
    "\n",
    "If you wish to train models using ml-agents, note that it is a very modular framework and that it may require some work to plug your own model in (depending on how far they are from PPO or SAC), but the performance gains are woth it! \n",
    "\n",
    "Well done! Now have a look at the **second notebook** on training an agent using animalai-train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
